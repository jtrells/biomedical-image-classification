{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference to ../src\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from dataset.MicroscopyTrainDataLoader import MicroscopyTrainDataLoader\n",
    "from experiments.microscopy.microscopy import experiment, get_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.CaptionModalityClassifier import CaptionModalityClassifier\n",
    "from models.MultiModalityClassifier import MultiModalityClassifier\n",
    "from dataset.CaptionDataModule import CaptionDataModule\n",
    "from utils.caption_utils import load_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shallow_model(model_id, model_dict):\n",
    "    model_name, experiment_name = model_id.split('.')\n",
    "    model = get_model(model_name, \"shallow\", 4, layers=model_dict[model_id]['layers'], pretrained=True)\n",
    "    \n",
    "    checkpoint = torch.load('../outputs/{0}/checkpoint.pt'.format(model_dict[model_id]['id']))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_INPUT_PATH = \"../src/experiments/microscopy/shallow-resnet50.json\"\n",
    "import json\n",
    "\n",
    "with open(JSON_INPUT_PATH) as json_file:\n",
    "    models = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_4_2 = load_shallow_model('resnet50.layer4-2', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = torch.rand([3,224,224])\n",
    "x_image = torch.stack([sample_image, sample_image], dim=0)\n",
    "x_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0799, -0.4657, -0.2040,  0.1104],\n",
       "        [ 0.0799, -0.4657, -0.2040,  0.1104]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_4_2(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_MODEL_PATH = \"./outputs/dainty-snowflake-10/checkpoint2.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_WORDS = 20000       # number of words to consider from embeddings vocabulary\n",
    "MAX_WORDS_PER_SENTENCE = 300   # sentence maximum length\n",
    "WORD_DIMENSION = 300           # number of features per embedding\n",
    "NUM_CLASSES = 4                # 4 microscopy classes\n",
    "\n",
    "DATA_PATH = '/workspace/data/multimodality_classification.csv'\n",
    "# EMBEDDINGS = '/workspace/data/embeddings'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dm = CaptionDataModule(BATCH_SIZE, DATA_PATH, MAX_NUMBER_WORDS, MAX_WORDS_PER_SENTENCE)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "# embeddings_dict = load_embedding_matrix(EMBEDDINGS, WORD_DIMENSION)\n",
    "\n",
    "# if dm.vocab_size < MAX_NUMBER_WORDS:\n",
    "#     MAX_NUMBER_WORDS = dm.vocab_size + 1\n",
    "# embedding_matrix = np.zeros((MAX_NUMBER_WORDS, WORD_DIMENSION))\n",
    "    \n",
    "# for word, idx in dm.word_index.items():    \n",
    "#     if idx < MAX_NUMBER_WORDS:\n",
    "#         word_embedding = embeddings_dict.get(word)\n",
    "#         if word_embedding is not None:\n",
    "#             embedding_matrix[idx] = word_embedding\n",
    "#         else:\n",
    "#             embedding_matrix[idx] = np.random.randn(WORD_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model = CaptionModalityClassifier(\n",
    "#                  max_input_length=MAX_WORDS_PER_SENTENCE,\n",
    "#                  vocab_size=MAX_NUMBER_WORDS,\n",
    "#                  embedding_dim=WORD_DIMENSION,\n",
    "#                  filters=100,\n",
    "#                  embeddings=embedding_matrix,\n",
    "#                  num_classes=NUM_CLASSES,\n",
    "#                  train_embeddings=True,\n",
    "#                  lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = CaptionModalityClassifier.load_from_checkpoint(checkpoint_path=TEXT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_text, y in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 300]), torch.Size([32, 300]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample = x_text[:2]\n",
    "# there is a bug when i send just one image\n",
    "x_sample.shape, x_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model(x_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = MultiModalityClassifier(text_model, resnet50_4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_4_2(x_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1857, -0.2383,  0.2918, -0.4302],\n",
       "        [ 0.0160, -0.2975,  0.4445, -0.3780]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi(x_sample, x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 300]), torch.Size([2, 2048]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features = text_model(x_sample)\n",
    "image_features = resnet50_4_2(x_image)\n",
    "text_features.shape, image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "fc = nn.Linear(300+2048, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.fc = nn.Identity()\n",
    "resnet50_4_2.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = text_features.view(text_features.size(0), -1)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([text_features, image_features], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2348])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1928,  0.0730, -0.1536,  0.5851],\n",
       "        [-0.0774, -0.1140, -0.1233,  0.5369]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
