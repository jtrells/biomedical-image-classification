{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Train a high level classifier\n",
    "The [ImageClef medical task dataset](https://www.imageclef.org/2016/medical) for subfigure classification comprises 30 different image modalities (types of images). In this notebook, we will classify those images in a *higher modality* by grouping some classes together. For example, the classes light microscopy (DMLI), electron microscopy (DMEL), transmission microscopy (DMTR) and fluorescence microscopy (DMFL) fall under the umbrella of the **microscopy** class. This grouping is already given in the file *higher_modality_vol1.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the code\n",
    "In this example, we will use our custom model ResNet which allows you to create a ResNet model 18, 34, 50, 101 or 151. Furthermore, it allows you to specify which layers you want to freeze and which ones to update. To load the data, we will use a DataModule that provides us with our training, validation and test datasets.\n",
    "\n",
    "Pytorch Lightning provides us with the Trainer to perform the traning and validation steps, an EarlyStopping class to implement this strategy and a wrapper to Weight and Biases to save our results on the cloud. You should notice that our DataModule is called Multimodality because it handles per each training sample the caption data and the figure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the path src as a module to enable imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "##########################################################\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from models.ResNetClassifier import ResNet\n",
    "from dataset.MultimodalityDataModule import MultimodalityDataModule       \n",
    "from utils.caption_utils import load_glove_matrix\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from dataset.utils import clean_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be feeding our model with a bunch of training samples specified in a CSV file. As you may know, Pytorch also allows to supply the training samples from memory or from a directory, but as we will build our datasets incrementally, I believe that a CSV file is more practical.\n",
    "\n",
    "The `PROJECT` variable matches the name at weight&biases, change this value to something like `learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My base path is /workspace/data\n",
      "The given dataset is located here: /workspace/data/higher_modality_vol1.csv\n",
      "The word embeddings are here: /workspace/data/higher_modality_vol1.csv, but we won't use them.\n",
      "Let's use this folder: outputs to save our artifacts.\n",
      "Finally, our image directory assumes that all the image paths in the CSV file are relative to this folder: /workspace/data/subfigure-classification\n"
     ]
    }
   ],
   "source": [
    "MAX_NUMBER_WORDS = 20000       # number of words to consider from embeddings vocabulary\n",
    "MAX_WORDS_PER_SENTENCE = 500   # sentence maximum length\n",
    "WORD_DIMENSION = 300           # number of features per embedding\n",
    "NUM_FILTERS = 128              # number of filters use in the ConvText module\n",
    "\n",
    "BASE_PATH = Path('/workspace/data')\n",
    "print(f\"My base path is {BASE_PATH}\")\n",
    "DATA_PATH = BASE_PATH / 'higher_modality_vol1.csv'\n",
    "print(f\"The given dataset is located here: {DATA_PATH}\")\n",
    "EMBEDDINGS = BASE_PATH / 'embeddings'\n",
    "print(f\"The word embeddings are here: {DATA_PATH}, but we won't use them.\")\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "print(f\"Let's use this folder: {OUTPUT_DIR} to save our artifacts.\")\n",
    "BASE_IMG_DIR = BASE_PATH / 'subfigure-classification'       # the image path in the CSV file are relative to this directory\n",
    "print(f\"Finally, our image directory assumes that all the image paths in the CSV file are relative to this folder: {BASE_IMG_DIR}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "PROJECT = None # fill this\n",
    "\n",
    "NUM_WORKERS = 72               # workers on the cluster, normally 2 or 4 on a laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The column **modality** matches the ImageCLEF category while the column **higher modality** links to our target class.\n",
    "For the images from ImageCLEF16 (source=clef16), you can notice that some of them contain the same caption. No worries, we will not be working with captions now but it's worth mentioning the these images are subfigures from an article figure, and for ImageCLEF16, the captions have not been separated per subfigure. The case is different for ImageCLEF13 (source=clef13) where the organizers did provide captions per sub-figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>modality</th>\n",
       "      <th>set</th>\n",
       "      <th>source</th>\n",
       "      <th>img_path</th>\n",
       "      <th>higher_modality</th>\n",
       "      <th>caption</th>\n",
       "      <th>split_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11373_2007_9226_Fig1_HTML-10.jpg</td>\n",
       "      <td>DMFL</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>clef16</td>\n",
       "      <td>2016/train/DMFL/11373_2007_9226_Fig1_HTML-10.jpg</td>\n",
       "      <td>MICROSCOPY</td>\n",
       "      <td>Colocalization of hNopp140, pol I and rDNA rep...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11373_2007_9226_Fig1_HTML-11.jpg</td>\n",
       "      <td>DMFL</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>clef16</td>\n",
       "      <td>2016/train/DMFL/11373_2007_9226_Fig1_HTML-11.jpg</td>\n",
       "      <td>MICROSCOPY</td>\n",
       "      <td>Colocalization of hNopp140, pol I and rDNA rep...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11373_2007_9226_Fig1_HTML-12.jpg</td>\n",
       "      <td>DMFL</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>clef16</td>\n",
       "      <td>2016/train/DMFL/11373_2007_9226_Fig1_HTML-12.jpg</td>\n",
       "      <td>MICROSCOPY</td>\n",
       "      <td>Colocalization of hNopp140, pol I and rDNA rep...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11373_2007_9226_Fig1_HTML-13.jpg</td>\n",
       "      <td>DMFL</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>clef16</td>\n",
       "      <td>2016/train/DMFL/11373_2007_9226_Fig1_HTML-13.jpg</td>\n",
       "      <td>MICROSCOPY</td>\n",
       "      <td>Colocalization of hNopp140, pol I and rDNA rep...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11373_2007_9226_Fig1_HTML-14.jpg</td>\n",
       "      <td>DMFL</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>clef16</td>\n",
       "      <td>2016/train/DMFL/11373_2007_9226_Fig1_HTML-14.jpg</td>\n",
       "      <td>MICROSCOPY</td>\n",
       "      <td>Colocalization of hNopp140, pol I and rDNA rep...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                img modality    set  source  \\\n",
       "0  11373_2007_9226_Fig1_HTML-10.jpg     DMFL  TRAIN  clef16   \n",
       "1  11373_2007_9226_Fig1_HTML-11.jpg     DMFL  TRAIN  clef16   \n",
       "2  11373_2007_9226_Fig1_HTML-12.jpg     DMFL  TRAIN  clef16   \n",
       "3  11373_2007_9226_Fig1_HTML-13.jpg     DMFL  TRAIN  clef16   \n",
       "4  11373_2007_9226_Fig1_HTML-14.jpg     DMFL  TRAIN  clef16   \n",
       "\n",
       "                                           img_path higher_modality  \\\n",
       "0  2016/train/DMFL/11373_2007_9226_Fig1_HTML-10.jpg      MICROSCOPY   \n",
       "1  2016/train/DMFL/11373_2007_9226_Fig1_HTML-11.jpg      MICROSCOPY   \n",
       "2  2016/train/DMFL/11373_2007_9226_Fig1_HTML-12.jpg      MICROSCOPY   \n",
       "3  2016/train/DMFL/11373_2007_9226_Fig1_HTML-13.jpg      MICROSCOPY   \n",
       "4  2016/train/DMFL/11373_2007_9226_Fig1_HTML-14.jpg      MICROSCOPY   \n",
       "\n",
       "                                             caption split_set  \n",
       "0  Colocalization of hNopp140, pol I and rDNA rep...     TRAIN  \n",
       "1  Colocalization of hNopp140, pol I and rDNA rep...     TRAIN  \n",
       "2  Colocalization of hNopp140, pol I and rDNA rep...     TRAIN  \n",
       "3  Colocalization of hNopp140, pol I and rDNA rep...     TRAIN  \n",
       "4  Colocalization of hNopp140, pol I and rDNA rep...     TRAIN  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset does not use all the classes from ImageCLEF, here we have only 27 of them because some classes have two few samples. Also notice that some images from Open-i (source=openi) have NaN values in the modality column. Still, not a problem as we will use the higher_modality column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['DMFL', 'DMTR', 'DRXR', 'DRUS', 'GFIG', 'D3DR', 'GGEL', 'DMEL',\n",
       "        'DRMR', 'DMLI', 'GTAB', 'DRCT', 'GSCR', 'GGEN', 'GCHE', 'GFLO',\n",
       "        'GSYS', 'GNCP', 'DVDM', 'GPLI', 'DSEM', 'DVOR', 'DSEE', 'DRAN',\n",
       "        'DVEN', 'DSEC', 'DRPE', nan], dtype=object),\n",
       " 28)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.modality.unique(), len(df.modality.unique())\n",
    "# df[pd.isnull(df.modality)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 different categories in the **higher modality** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "higher_modality\n",
       "EXPERIMENTAL     614\n",
       "GRAPHICS        5387\n",
       "MICROSCOPY      3219\n",
       "MOLECULAR        805\n",
       "ORGANISMS       3547\n",
       "OTHER            796\n",
       "Name: img, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mods = df.groupby('higher_modality')['img'].count()\n",
    "count_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data module handles text and images, we need to pass variables like the length of each sentence (for padding purposes), the maximum size of our vocabulary, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current vocabulary size: 20197\n"
     ]
    }
   ],
   "source": [
    "SEED = 443\n",
    "dm = MultimodalityDataModule(BATCH_SIZE,\n",
    "                        str(DATA_PATH),\n",
    "                        MAX_NUMBER_WORDS,\n",
    "                        MAX_WORDS_PER_SENTENCE,\n",
    "                        str(BASE_IMG_DIR),\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        seed=SEED,\n",
    "                        caption_col='caption',\n",
    "                        modality_col='higher_modality',\n",
    "                        target_class_col='split_set',\n",
    "                        path_col='img_path')\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "print(\"Current vocabulary size: {0}\".format(dm.vocab_size))\n",
    "train_dataloader = dm.train_dataloader()\n",
    "train_dataset    = train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the elements of the training dataset by index; this is how the training loop will get this data. Each value will be a Tensor of 3 values: text_caption, image_values, class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1056,\n",
       "             2, 1437, 1572,   68,    3,  877, 4073,    6,  159,    2, 1437,    3,\n",
       "          1572, 1250,    1, 3082,  331, 1271,   13,   11,  592,   17,  653,  400,\n",
       "            15, 3643, 5281, 1235,  287,   51,   78,    8, 5282,  208,    8, 4538,\n",
       "             5,    1, 2305,   13,   11,   21, 2597,  213,   51,    5,   13,   11,\n",
       "           844,    9,  490,  625, 1545,    2, 1572,   68,    3, 3174,    1,  128,\n",
       "            19, 1647,    7, 3536, 5283,   10,  159,    2, 1437,    3, 5284,  653,\n",
       "            13,   21, 2597,   11,  703,    4, 1546,  223,    7,    6, 1475,  135,\n",
       "           877,  220,    3,    6, 4790, 4791,   55, 3537, 5285,    1,  123,   13,\n",
       "            11,  174,  593,   87,    7,   69, 1437, 5286,   83, 5287, 3644,   12,\n",
       "           877, 3538,    7,    1, 5288,  107, 3174,  653,   13,  168,    7,  845,\n",
       "          1236,  803,  432, 1437,   11, 1704,    8, 3083,    1,  877,    3,    1,\n",
       "           845, 1236, 1596,   79,   41, 2201,   29,   37]]),\n",
       " tensor([[[-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314],\n",
       "          [-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314],\n",
       "          [-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314],\n",
       "          ...,\n",
       "          [-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314],\n",
       "          [-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314],\n",
       "          [-1.3314, -1.3314, -1.3314,  ..., -1.3314, -1.3314, -1.3314]],\n",
       " \n",
       "         [[-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326],\n",
       "          [-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326],\n",
       "          [-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326],\n",
       "          ...,\n",
       "          [-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326],\n",
       "          [-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326],\n",
       "          [-1.3326, -1.3326, -1.3326,  ..., -1.3326, -1.3326, -1.3326]],\n",
       " \n",
       "         [[-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960],\n",
       "          [-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960],\n",
       "          [-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960],\n",
       "          ...,\n",
       "          [-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960],\n",
       "          [-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960],\n",
       "          [-1.2960, -1.2960, -1.2960,  ..., -1.2960, -1.2960, -1.2960]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader codes the class name into a integer value. To see the correspondence, you can use the label encoder in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EXPERIMENTAL', 'GRAPHICS', 'MICROSCOPY', 'MOLECULAR', 'ORGANISMS',\n",
       "       'OTHER'], dtype='<U12')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we were feeding also the caption data, we would need an embedding matrix to create our word vectors.\n",
    "# Thus, we can ignore these lines for now.\n",
    "\n",
    "# if dm.vocab_size < MAX_NUMBER_WORDS:\n",
    "#     MAX_NUMBER_WORDS = dm.vocab_size\n",
    "\n",
    "# embeddings_matrix = load_glove_matrix(EMBEDDINGS, WORD_DIMENSION, MAX_NUMBER_WORDS, dm.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.higher_modality.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.38888889, 0.52605967, 0.72953294, 3.14067278, 0.56028369,\n",
       "       2.51715686])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.class_weights # You can use class weights to balance the loss function. Sometimes they help but it's up to you to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Create a ResNet50 model with values pre-trained from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(df.higher_modality.unique())\n",
    "model = ResNet('resnet50',\n",
    "               num_classes,\n",
    "               pretrained=True,\n",
    "               fine_tuned_from=\"whole\",\n",
    "               lr=5e-6,\n",
    "               class_weights=dm.class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wand_logger` object connects your code with W&B. Moreover, in the model definition, every hyperparameter is persisted as part of the run. Once you save this hyperparameters in your model, you can use them from `self.hparams` (check ResNetClassifier class). If you want to avoid syncing with W&B, send `None` to logger; you can also modify the code to use TensorBoard instead.\n",
    "\n",
    "Note: If W&B notifies you that there is a new version, try installing it using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/jtrells/biomedical-multimodal\" target=\"_blank\">https://app.wandb.ai/jtrells/biomedical-multimodal</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/jtrells/biomedical-multimodal/runs/3g0o3961\" target=\"_blank\">https://app.wandb.ai/jtrells/biomedical-multimodal/runs/3g0o3961</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ResNet           | 23 M  \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous-dream-235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69277f1fed14e259308bdd8c956435c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the context for the weight and biases logger\n",
    "wandb_logger = WandbLogger(project=PROJECT, tags=['nb', 'higher-modality'])\n",
    "wandb_logger.experiment.save()\n",
    "print(wandb_logger.experiment.name)\n",
    "\n",
    "output_run_path = OUTPUT_DIR / wandb_logger.experiment.name \n",
    "os.makedirs(output_run_path, exist_ok=False)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "#default_root_dir=str(output_run_path),\n",
    "trainer = Trainer(gpus=1,\n",
    "                  max_epochs=2,                  \n",
    "                  early_stop_callback=early_stop_callback,\n",
    "                  logger=wandb_logger)\n",
    "trainer.fit(model, dm)\n",
    "trainer.save_checkpoint(str(output_run_path / 'final.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get a random-generated name from W&B and we are saving the model as `final.pt`. Hence, we can use this path to load the model from that checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/sage-cosmos-234/final.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class_weights\":   [4.38888889 0.52605967 0.72953294 3.14067278 0.56028369 2.51715686]\n",
       "\"fine_tuned_from\": whole\n",
       "\"lr\":              5e-06\n",
       "\"name\":            resnet50\n",
       "\"num_classes\":     6\n",
       "\"pretrained\":      True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning has a handy method to evaluate your model. Internally, it executes the `test_step` and `test_step_end` methods from the Model definition. In this example, we are passing the validation data for evaluation (which is also used on the training - validation loop) but you can pass any dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de768cf489f94b70b9d71473d4f4ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.08272552490234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.97      0.84        78\n",
      "           1       0.97      0.83      0.90       651\n",
      "           2       0.95      0.81      0.87       469\n",
      "           3       0.61      0.83      0.71       109\n",
      "           4       0.91      0.96      0.93       612\n",
      "           5       0.46      0.71      0.56       136\n",
      "\n",
      "    accuracy                           0.86      2055\n",
      "   macro avg       0.78      0.85      0.80      2055\n",
      "weighted avg       0.89      0.86      0.87      2055\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(86.0827, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 86.08272552490234}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1, logger=None)\n",
    "trainer.test(model, test_dataloaders=dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EXPERIMENTAL', 'GRAPHICS', 'MICROSCOPY', 'MOLECULAR', 'ORGANISMS',\n",
       "       'OTHER'], dtype='<U12')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader().dataset.le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to improve?\n",
    "\n",
    "1. In this example, we used a dataloader that handles text and image data but only used the image tensors. If you see the ResNetClassifier class, the steps are extracting two values from each batch and discarding the third one (`_, x, y = batch`). We do this because we don't want the first value that represents the tokenized captions. As you are going to be managing only images, create a new DataModule and DataSet class to just handle images.\n",
    "\n",
    "    * DataModule can also handle data partitioning to create a train/val split. In our code, we already have these values in a column of the dataset. Hence, the dataset class is responsible for this filtering step.\n",
    "\n",
    "2. For your image classifiers, consider only using image features. You can create a new ResNet classifier or other architectures and modify how each step process the data. Technically, you can opt to check the size of the tuple but that validation may add some overhead.\n",
    "\n",
    "3. Compare your experiments to get a high modality classifier.\n",
    "\n",
    "4. Explore the dataset to handle only MRIs, CTs, and X-Rays.\n",
    "\n",
    "5. Search for open access datasets that can expand the number of samples. For example, check src/utils for the download_ct and download_xray scripts. Download and analyze the data. Keep track of each dataset on a table and identify their characteristics (e.g. many datasets may only focus on chest images).\n",
    "\n",
    "6. Let's move the data to /mnt/data and /mnt/artifacts where the artifacts are the saved models that you create. Update accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
